{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4913bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.6.1)\n",
      "Collecting Pillow\n",
      "  Using cached pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pillow-11.2.1-cp312-cp312-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "Installing collected packages: Pillow\n",
      "Successfully installed Pillow-11.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas scikit-learn Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5fa84b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import sys\n",
    "from PIL import Image\n",
    "\n",
    "# Definir caminhos\n",
    "DATA_PATH = 'data/'\n",
    "ARCHIVE_PATH = 'archive/' \n",
    "\n",
    "# Definir proporções de divisão\n",
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1 # A soma deve ser 1.0\n",
    "\n",
    "# Definir tamanho alvo da imagem\n",
    "TARGET_WIDTH = 256  \n",
    "TARGET_HEIGHT = 256 \n",
    "\n",
    "# Definir mapeamentos de diretório (inverso do mapeamento)\n",
    "VIEW_POSITION_DIR_MAP = {0: 'PA', 1: 'AP'}\n",
    "FINDING_LABELS_DIR_MAP = {0: '0', 1: '1'} # Usando '0' e '1' como nomes de pastas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcbcca55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o processo de cópia e organização de imagens com redimensionamento...\n",
      "As imagens serão redimensionadas para 256x256 pixels.\n",
      "Carregando dados processados de 'data/data_entry.csv'...\n",
      "Carregadas 26634 entradas.\n",
      "Colunas: ['Image Index', 'Finding Labels', 'Patient Age', 'Patient Gender', 'View Position']\n",
      "Contagem de valores para 'Finding Labels':\n",
      " Finding Labels\n",
      "1    13317\n",
      "0    13317\n",
      "Name: count, dtype: int64\n",
      "Contagem de valores para 'View Position':\n",
      " View Position\n",
      "1    13456\n",
      "0    13178\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Iniciando o processo de cópia e organização de imagens com redimensionamento...\")\n",
    "print(f\"As imagens serão redimensionadas para {TARGET_WIDTH}x{TARGET_HEIGHT} pixels.\")\n",
    "\n",
    "# --- Carregar os dados processados ---\n",
    "data_entry_filepath = os.path.join(DATA_PATH, 'data_entry.csv')\n",
    "\n",
    "if not os.path.exists(data_entry_filepath):\n",
    "    print(f\"Erro: Arquivo de dados processados não encontrado em '{data_entry_filepath}'.\")\n",
    "    print(\"Por favor, execute o notebook data_extraction.ipynb primeiro para gerar este arquivo.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"Carregando dados processados de '{data_entry_filepath}'...\")\n",
    "try:\n",
    "    processed_df = pd.read_csv(data_entry_filepath)\n",
    "    print(f\"Carregadas {len(processed_df)} entradas.\")\n",
    "    print(\"Colunas:\", processed_df.columns.tolist())\n",
    "    print(\"Contagem de valores para 'Finding Labels':\\n\", processed_df['Finding Labels'].value_counts())\n",
    "    print(\"Contagem de valores para 'View Position':\\n\", processed_df['View Position'].value_counts())\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar dados processados: {e}\")\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22ea3812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Escaneando 'archive/' em busca de arquivos de imagem...\n",
      "Escaneamento finalizado. Total de arquivos de imagem únicos encontrados no arquivo: 112120\n"
     ]
    }
   ],
   "source": [
    "# --- Construir um mapa de 'Image Index' para o caminho da fonte original ---\n",
    "print(f\"\\nEscaneando '{ARCHIVE_PATH}' em busca de arquivos de imagem...\")\n",
    "image_path_map = {}\n",
    "image_count = 0\n",
    "# Escanear diretórios images_001 a images_012\n",
    "for i in range(1, 13):\n",
    "    image_dir = os.path.join(ARCHIVE_PATH, f'images_{i:03d}', 'images')\n",
    "    if os.path.exists(image_dir):\n",
    "        # Usar glob para listagem eficiente de arquivos\n",
    "        files_in_dir = glob.glob(os.path.join(image_dir, '*.png'))\n",
    "        for f in files_in_dir:\n",
    "             image_path_map[os.path.basename(f)] = f\n",
    "             image_count += 1\n",
    "        # print(f\"Escaneado {image_dir} ({len(files_in_dir)} arquivos encontrados). Total de arquivos mapeados: {image_count}\") # Escaneamento detalhado\n",
    "    else:\n",
    "        print(f\"Aviso: Diretório '{image_dir}' não encontrado. Ignorando.\")\n",
    "\n",
    "\n",
    "print(f\"Escaneamento finalizado. Total de arquivos de imagem únicos encontrados no arquivo: {len(image_path_map)}\")\n",
    "\n",
    "# Adicionar o caminho da fonte ao dataframe\n",
    "processed_df['source_path'] = processed_df['Image Index'].map(image_path_map)\n",
    "\n",
    "# Verificar imagens no dataframe, mas não encontradas no arquivo (não deve haver nenhuma se o notebook for executado corretamente)\n",
    "missing_images = processed_df[processed_df['source_path'].isnull()]\n",
    "if not missing_images.empty:\n",
    "    print(f\"\\nAviso: {len(missing_images)} imagens listadas em '{data_entry_filepath}' não foram encontradas no arquivo:\")\n",
    "    print(missing_images['Image Index'].tolist()[:10], \"...\") # Imprimir as 10 primeiras\n",
    "    # descartar as ausentes para este script\n",
    "    processed_df.dropna(subset=['source_path'], inplace=True)\n",
    "    print(f\"Removidas {len(missing_images)} entradas devido a arquivos de imagem ausentes. Entradas restantes: {len(processed_df)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e5716c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dividindo dados em conjuntos de treino, validação e teste...\n",
      "Tamanho do conjunto de treino: 21307\n",
      "Tamanho do conjunto de validação: 2663\n",
      "Tamanho do conjunto de teste: 2664\n",
      "\n",
      "Distribuição de 'Finding Labels' no conjunto de treino:\n",
      " Finding Labels\n",
      "1    0.500023\n",
      "0    0.499977\n",
      "Name: proportion, dtype: float64\n",
      "Distribuição de 'Finding Labels' no conjunto de validação:\n",
      " Finding Labels\n",
      "0    0.500188\n",
      "1    0.499812\n",
      "Name: proportion, dtype: float64\n",
      "Distribuição de 'Finding Labels' no conjunto de teste:\n",
      " Finding Labels\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# --- Dividir os dados em conjuntos de treino, validação e teste ---\n",
    "print(\"\\nDividindo dados em conjuntos de treino, validação e teste...\")\n",
    "\n",
    "# Usar estratificação para manter a distribuição de 'Finding Labels' e 'View Position'\n",
    "# Criar uma coluna combinada para estratificação\n",
    "processed_df['stratify_col'] = processed_df['Finding Labels'].astype(str) + '_' + processed_df['View Position'].astype(str)\n",
    "\n",
    "# Divisão inicial: Treino e Temp (Validação + Teste)\n",
    "train_df, temp_df = train_test_split(\n",
    "    processed_df,\n",
    "    test_size=(VAL_RATIO + TEST_RATIO), # 0.1 + 0.1 = 0.2\n",
    "    random_state=42,\n",
    "    stratify=processed_df['stratify_col']\n",
    ")\n",
    "\n",
    "# Segunda divisão: Validação e Teste a partir de Temp\n",
    "# Calcular o tamanho do teste relativo ao conjunto temporário\n",
    "relative_test_size = TEST_RATIO / (VAL_RATIO + TEST_RATIO) # 0.1 / 0.2 = 0.5\n",
    "\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=relative_test_size, # 0.5\n",
    "    random_state=42,\n",
    "    stratify=temp_df['stratify_col']\n",
    ")\n",
    "\n",
    "# Descartar a coluna de estratificação temporária dos dataframes divididos\n",
    "train_df = train_df.drop(columns='stratify_col')\n",
    "val_df = val_df.drop(columns='stratify_col')\n",
    "test_df = test_df.drop(columns='stratify_col')\n",
    "\n",
    "print(f\"Tamanho do conjunto de treino: {len(train_df)}\")\n",
    "print(f\"Tamanho do conjunto de validação: {len(val_df)}\")\n",
    "print(f\"Tamanho do conjunto de teste: {len(test_df)}\")\n",
    "\n",
    "# Verificar divisões\n",
    "print(\"\\nDistribuição de 'Finding Labels' no conjunto de treino:\\n\", train_df['Finding Labels'].value_counts(normalize=True))\n",
    "print(\"Distribuição de 'Finding Labels' no conjunto de validação:\\n\", val_df['Finding Labels'].value_counts(normalize=True))\n",
    "print(\"Distribuição de 'Finding Labels' no conjunto de teste:\\n\", test_df['Finding Labels'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad21153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Criando diretórios de destino...\n",
      "Diretório criado/garantido: data/train/PA/0\n",
      "Diretório criado/garantido: data/train/PA/1\n",
      "Diretório criado/garantido: data/train/AP/0\n",
      "Diretório criado/garantido: data/train/AP/1\n",
      "Diretório criado/garantido: data/validation/PA/0\n",
      "Diretório criado/garantido: data/validation/PA/1\n",
      "Diretório criado/garantido: data/validation/AP/0\n",
      "Diretório criado/garantido: data/validation/AP/1\n",
      "Diretório criado/garantido: data/test/PA/0\n",
      "Diretório criado/garantido: data/test/PA/1\n",
      "Diretório criado/garantido: data/test/AP/0\n",
      "Diretório criado/garantido: data/test/AP/1\n",
      "Estrutura de diretórios de destino criada.\n"
     ]
    }
   ],
   "source": [
    "# --- Criar diretórios de destino ---\n",
    "print(\"\\nCriando diretórios de destino...\")\n",
    "\n",
    "split_dirs = {'train': train_df, 'validation': val_df, 'test': test_df}\n",
    "\n",
    "for split_name in split_dirs.keys():\n",
    "    for view_pos_val, view_pos_str in VIEW_POSITION_DIR_MAP.items():\n",
    "        for finding_label_val, finding_label_str in FINDING_LABELS_DIR_MAP.items():\n",
    "            dest_dir = os.path.join(DATA_PATH, split_name, view_pos_str, finding_label_str)\n",
    "            os.makedirs(dest_dir, exist_ok=True)\n",
    "            print(f\"Diretório criado/garantido: {dest_dir}\") # Descomentar para criação detalhada\n",
    "\n",
    "print(\"Estrutura de diretórios de destino criada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f72d32dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Redimensionando e copiando imagens para a nova estrutura...\n",
      "Este processo pode levar algum tempo dependendo do número de imagens e do tamanho alvo.\n",
      "Processando 21307 arquivos para a divisão: train\n",
      "  Processados 500/26634 arquivos...\n",
      "  Processados 1000/26634 arquivos...\n",
      "  Processados 1500/26634 arquivos...\n",
      "  Processados 2000/26634 arquivos...\n",
      "  Processados 2500/26634 arquivos...\n",
      "  Processados 3000/26634 arquivos...\n",
      "  Processados 3500/26634 arquivos...\n",
      "  Processados 4000/26634 arquivos...\n",
      "  Processados 4500/26634 arquivos...\n",
      "  Processados 5000/26634 arquivos...\n",
      "  Processados 5500/26634 arquivos...\n",
      "  Processados 6000/26634 arquivos...\n",
      "  Processados 6500/26634 arquivos...\n",
      "  Processados 7000/26634 arquivos...\n",
      "  Processados 7500/26634 arquivos...\n",
      "  Processados 8000/26634 arquivos...\n",
      "  Processados 8500/26634 arquivos...\n",
      "  Processados 9000/26634 arquivos...\n",
      "  Processados 9500/26634 arquivos...\n",
      "  Processados 10000/26634 arquivos...\n",
      "  Processados 10500/26634 arquivos...\n",
      "  Processados 11000/26634 arquivos...\n",
      "  Processados 11500/26634 arquivos...\n",
      "  Processados 12000/26634 arquivos...\n",
      "  Processados 12500/26634 arquivos...\n",
      "  Processados 13000/26634 arquivos...\n",
      "  Processados 13500/26634 arquivos...\n",
      "  Processados 14000/26634 arquivos...\n",
      "  Processados 14500/26634 arquivos...\n",
      "  Processados 15000/26634 arquivos...\n",
      "  Processados 15500/26634 arquivos...\n",
      "  Processados 16000/26634 arquivos...\n",
      "  Processados 16500/26634 arquivos...\n",
      "  Processados 17000/26634 arquivos...\n",
      "  Processados 17500/26634 arquivos...\n",
      "  Processados 18000/26634 arquivos...\n",
      "  Processados 18500/26634 arquivos...\n",
      "  Processados 19000/26634 arquivos...\n",
      "  Processados 19500/26634 arquivos...\n",
      "  Processados 20000/26634 arquivos...\n",
      "  Processados 20500/26634 arquivos...\n",
      "  Processados 21000/26634 arquivos...\n",
      "Processando 2663 arquivos para a divisão: validation\n",
      "  Processados 21500/26634 arquivos...\n",
      "  Processados 22000/26634 arquivos...\n",
      "  Processados 22500/26634 arquivos...\n",
      "  Processados 23000/26634 arquivos...\n",
      "  Processados 23500/26634 arquivos...\n",
      "Processando 2664 arquivos para a divisão: test\n",
      "  Processados 24000/26634 arquivos...\n",
      "  Processados 24500/26634 arquivos...\n",
      "  Processados 25000/26634 arquivos...\n",
      "  Processados 25500/26634 arquivos...\n",
      "  Processados 26000/26634 arquivos...\n",
      "  Processados 26500/26634 arquivos...\n",
      "\n",
      "Processamento e cópia de imagens completos. Total de arquivos processados: 26634\n",
      "Processo finalizado.\n"
     ]
    }
   ],
   "source": [
    "# --- Redimensionar e copiar imagens para a nova estrutura ---\n",
    "print(\"\\nRedimensionando e copiando imagens para a nova estrutura...\")\n",
    "print(\"Este processo pode levar algum tempo dependendo do número de imagens e do tamanho alvo.\")\n",
    "\n",
    "processed_count = 0\n",
    "total_files_to_process = len(processed_df)\n",
    "\n",
    "for split_name, df in split_dirs.items():\n",
    "    print(f\"Processando {len(df)} arquivos para a divisão: {split_name}\")\n",
    "    for index, row in df.iterrows():\n",
    "        image_index = row['Image Index']\n",
    "        source_path = row['source_path']\n",
    "        finding_label_val = row['Finding Labels']\n",
    "        view_pos_val = row['View Position']\n",
    "\n",
    "        view_pos_str = VIEW_POSITION_DIR_MAP[view_pos_val]\n",
    "        finding_label_str = FINDING_LABELS_DIR_MAP[finding_label_val] # Será '0' ou '1'\n",
    "\n",
    "        dest_dir = os.path.join(DATA_PATH, split_name, view_pos_str, finding_label_str)\n",
    "        dest_path = os.path.join(dest_dir, image_index) # Manter o nome do arquivo original\n",
    "\n",
    "        # Realizar redimensionamento e salvamento\n",
    "        try:\n",
    "            with Image.open(source_path) as img:\n",
    "                # Redimensionar a imagem usando o filtro LANCZOS para uma boa qualidade de redução\n",
    "                resized_img = img.resize((TARGET_WIDTH, TARGET_HEIGHT), Image.Resampling.LANCZOS)\n",
    "\n",
    "                # Salvar a imagem redimensionada no caminho de destino\n",
    "                resized_img.save(dest_path, format='PNG')\n",
    "\n",
    "            processed_count += 1\n",
    "            if processed_count % 500 == 0: # Imprimir progresso a cada 500 arquivos\n",
    "                print(f\"  Processados {processed_count}/{total_files_to_process} arquivos...\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             print(f\"  Erro: Arquivo fonte não encontrado para {image_index} em {source_path}. Ignorando.\")\n",
    "        except Exception as e:\n",
    "             print(f\"  Erro ao processar {image_index} de {source_path} para {dest_path}: {e}\")\n",
    "\n",
    "\n",
    "print(f\"\\nProcessamento e cópia de imagens completos. Total de arquivos processados: {processed_count}\")\n",
    "print(\"Processo finalizado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd934a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
